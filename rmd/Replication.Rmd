---
title: "Replication of BKR"
author: "Shunsuke Matsuno"
date: "2020/6/1"
output:
  html_document:
    code_folding: show
    df_print: paged
    number_sections: no
    theme: united
    toc: yes
    toc_float: no
  pdf_document:
    toc: yes
---

```{r settings, echo=FALSE}
### Setting the global code chunk options ###
# Args
#   comment='': won't append any string to the start of each line of results
#   fig.align='center': align figures to the center of document
knitr::opts_chunk$set(comment="", fig.align="center", warning = FALSE)
```
# Required libraries
```{r}
easypackages::libraries('tidyverse', 'ggplot2','NumbersGame')
```


# Introduction
- This note replicates Bird, Karolyi, and Ruchti (BKR; 2019, JAE).

# Simulating the data
- Since I don't have access to the original data, I simulate the "observed" data using the estimated parameters in BKR.

## Parameters
- $\theta = (\eta, \gamma, \psi^2, \zeta)$
```{r}
set.seed(2020)

# Table 4
eta <- 0.5  # marginal cost
gamma <- 2.0817    # curvature
psi2 <- 0.8201   # variance
zeta <- 3.7105    # variance multiplier
theta <- c(eta, gamma, psi2, zeta)
```
## Benefit function
- Since the estimates of full model (coefficients on polynomials) are not reported, I approximate the capital market benefit function by `compute_benefit.R`.

## Latent earnings distribution
- I assume that latent earnings distribution is generated by normal distribution.
- For the computational ease, I set the number of firms (observations) to $10,000$.
    - In BKR, the number of observations is $49,604$.
- I use the following multinomial distribution as a discretized normal:

$$
e \sim \textit{Multi}(N, 1, P)\\
\mathcal{X} = \{ -20,-19,\dots,0,\dots,19,20 \} \\
P = (\phi(-20),\dots,\phi(20))\qquad (\text{normilized})
$$

```{r,cache=TRUE}
# number of firms
N <- 100000
sigma <- 5

df_firm <- simulate_firm(S = N, sigma)

ggplot(df_firm, aes(x = e)) +
  geom_bar() +
  xlim(-21, 21)

```

## Manipulation
- Each firm decides the amount of manipulation, $m$, according to the maximization problem.
- `compute_utility` computes the following utility:
$$
u_{\text {Discrete}}(b, m, \theta)=\sum_{-20}^{20} \phi_{m, \theta}(\varepsilon) \mathcal{B}(b+m+\varepsilon) d \varepsilon-\beta_{b} m^{\gamma}
$$
- Since I don't have the original data, I am assuming the following benefit function.
    - There is a jump by $1.5$ at $R=0$.
    
$$
\mathcal{B}(R)=\begin{cases}
0.05R-1 & R<0\\
0.05R+0.5 & R\ge0
\end{cases}
$$

```{r, cache=TRUE}
df_firm_utility <- compute_optimal_manipulation_simulated(df_firm, theta, S = N)
df_firm_utility
```

```{r}
# plot
plot_latent_reported(df_firm_utility)
```

- The 'empirical' distribution of earnings surprises, $R$, is a bit drastic than the actual empirical distribution.
    - Compare the following graph with Fig.2.
    - This is because my simulation assumes that the empirical distribution is solely determined by the optimization behavior of BKR's model, while in reality variety of other factors come into play.
    
```{r}
qplot(df_firm_utility$R, geom = 'bar', xlim = c(-11,11), xlab = 'reported')
```



# Estimation
- Regarding the simulated data as 'observed' data, I recover the parameter $\theta$ by BKR method.
- See `SummaryBKR.pdf` for the step-by-step estimation procedure.
```{r}
df_observed <- df_firm_utility %>%      # df_firm_utility is the 'true' data
  select(firm, R)                       # Discard the information that cannot be observed (by researcher)
df_observed

# save data
save(df_observed, file = '../output/data/df_observed.RData')

# load data
load(file = '../output/data/df_observed.RData')
```
## Step 1. Candidate $\theta$
- I set the initial $\theta$ to be the true $\theta$.
```{r}
theta_init <- theta
```

## Step 2. Simulate optimal firm behavior
- I compute the transition matrix, $P$.
- Considering the computational cost, I set the number of simulations $S$ to be $1,000$.
    - The original paper uses $S=10,000$.
  
```{r, message=FALSE}
S <- 1000   # number of simulations

P <- compute_P(theta_init, S)
```


## Step 3. Recover the latent earnigs distribution
- I recover the latent earnings distribution by inverting $P$.
- Since some bins do not contain any firms, I have to make rows for such bins.
- Observed bins that are outside $[-20,20]$ is excluded from the data.
```{r}
df_pi <- compute_pi(df_observed)
```


- Then, we can invert and obtain the latent distribution.

```{r}
df_x <- compute_x(P, df_pi)
```


- Plot to compare $x$ and $\pi$.

```{r}
df_pi_x <- full_join(df_pi, df_x, by = 'bin') %>% 
  pivot_longer(-bin, names_to = 'type', values_to = 'freq')

g <- ggplot(df_pi_x, aes(x = bin, y = freq, fill = type)) +
  geom_bar(aes(fill = type), stat = 'identity', position = 'dodge', alpha =1 )+
  xlim(-20,20) +
  scale_fill_hue(name = "type", labels = c(count_e = "inverted latent", count_R ='reported'))

plot(g)
```

## Step 4 Optimizetion
- Lastly, using GMM (more precisely, SMM), I estimate the parameter.
- The objective function is 
$$
L(\theta) = \left[\left(\begin{array}{l}
\pi\\
\boldsymbol{x}_{S}(\theta)_{(2,\ldots T)}
\end{array}\right)-\left(\begin{array}{l}
\boldsymbol{x}_{S}(\theta)\\
\boldsymbol{x}_{S}(\theta)_{(1,\ldots T-1)}
\end{array}\right)\right]^{\prime}\Omega\left[\left(\begin{array}{l}
\pi\\
\boldsymbol{x}_{S}(\theta)_{(2,\ldots T)}
\end{array}\right)-\left(\begin{array}{l}
\boldsymbol{x}_{S}(\theta)\\
\boldsymbol{x}_{S}(\theta)_{(1,\ldots T-1)}
\end{array}\right)\right]
$$
```{r}
GMM_objective(theta_init, df_observed)
```

- Finally, I optimize the GMM objective function.
    - In optimization, the number of simulation, $S$, should be at least $100,000$ to get a reasonable result.
    - However, since this optimization takes a lot of time, I set $S = 10,000$.
```{r GMM, cache=TRUE}
# # Nelder-Mead
# optim_result <- optim(par = c(0.5, 2, 1, 4),
#                       fn = GMM_objective,
#                       df_observed = df_observed,
#                       S = 10000,
#                       method = 'Nelder-Mead')
# 
# # optim_result <- optim(par = c(0.5, 2, 1, 4),
# #                       fn = GMM_objective,
# #                       df_observed = df_observed,
# #                       method = 'L-BFGS-B',
# #                       lower = c(0.001, 1, 0, 0),
# #                       upper = c(2, 4, 2, 5),
# #                       control = list(maxit = 50))
# optim_result
```

- Let's see the shape of objective function w.r.t. each parameter.
```{r, cache=TRUE}
S <- 50000

eta <- seq(0.2, 1, .05)
plot_objFunc(theta, 1, eta, df_observed, S = S)

gamma <- seq(1, 5, .1)
plot_objFunc(theta, 2, gamma, df_observed, S = S)

psi2 <- seq(0.1, 1.5, .1)
plot_objFunc(theta, 3, psi2, df_observed, S = S)

zeta <- seq(3, 5, .1)
plot_objFunc(theta, 4, zeta, df_observed, S = S)

```

##### Computational burdern of BKR method
- Let's see the time it take for each iteration given the number of simulations.

```{r}
S <- 1000
system.time(GMM_objective(theta_init, df_observed, S))

S <- 10000
system.time(GMM_objective(theta_init, df_observed, S))

S <- 100000
system.time(GMM_objective(theta_init, df_observed, S))
```



### Original simulated obj
- The number of simulation $S$ should be at least $100,000$ to get reasonable result.
    - We need to specify the variance, which I assume to 5
```{r}
# simulate data
S <- 100000
df_simulated <- simulate_firm(sigma, S)

# # optimization
# optim_result <- optim(par = c(0.3, 1.5, 1, 3),
#                       fn = CMD_obj,
#                       df_observed = df_observed,
#                       df_simulated = df_simulated,
#                       S = S,
#                       method = 'Nelder-Mead')
# optim_result
```
    

```{r, cache=TRUE}
graphs_CMD <- list()

eta <- seq(0.2, 1, .05)
graphs_CMD[[1]] <- plot_objFunc_CMD(theta, 1, eta,  df_observed = df_observed, df_simulated = df_simulated, S = S, parallel = TRUE)

gamma <- seq(1, 5, .2)
graphs_CMD[[2]] <- plot_objFunc_CMD(theta, 2, gamma, df_simulated, df_observed,  S = S, parallel = TRUE)

psi2 <- seq(0.1, 1.5, .1)
graphs_CMD[[3]] <- plot_objFunc_CMD(theta, 3, psi2, df_simulated, df_observed,  S = S, parallel = TRUE)

zeta <- seq(2, 5, .1)
graphs_CMD[[4]] <- plot_objFunc_CMD(theta, 4, zeta, df_simulated, df_observed,  S = S, parallel = TRUE)

for(i in 1:4){
  ggsave(plot = graphs_CMD[[i]], file = paste0( '../output/plots/CMD_objectives_', i, '.png'))
}
```

##### Computational burdern of BKR method
- Let's see the time it take for each iteration given the number of simulations
```{r}
S <- 1000
df_simulated <- simulate_firm(sigma, S)
system.time(CMD_obj(theta, df_observed, df_simulated, S))

S <- 10000
df_simulated <- simulate_firm(sigma, S)
system.time(CMD_obj(theta, df_observed, df_simulated, S))

S <- 100000
df_simulated <- simulate_firm(sigma, S)
system.time(CMD_obj(theta, df_observed, df_simulated, S))
```

#### parallel
```{r}
# simulate data
S <- 100
df_simulated <- simulate_firm(sigma, S)

eta <- seq(0.2, 1, .05)

# store the values of obj. func. to `obj`
obj <- numeric(length(eta))

# compute onj. func. for each parameter
library(doParallel)
cl <- makePSOCKcluster(4)
doParallel::registerDoParallel(cl)


system.time(
obj <- foreach (i = seq_along(eta),
        .packages = c('dplyr', 'purrr', 'NumbersGame'),
        .combine = 'rbind') %dopar% {
  theta_temp <- theta
  theta_temp[1] <- eta[i]
  NumbersGame::CMD_obj(theta, df_observed, df_simulated, S)
}
)
qplot(eta, obj)

stopCluster(cl)
```


## Counterfacturals 
- See what happens if we increase the marginal cost parameter, $\eta$.
    - Since manipulation becomes more costly, firms that find manipulation beneficial are limited to just below the zero threshold.
- Since $\eta = 0.5$ gives distribution more suited to observed data in BKR, in what follows I use $\eta = 0.5$ as a true value.
```{r, cache=TRUE}
theta_new <- theta
theta_new[1] <- 2

df_firm_utility_new <- compute_optimal_manipulation_simulated(df_firm, theta = theta_new, S = N)
plot_latent_reported(df_firm_utility_new)
```
- Compare the reported distribution to the distribution with $\eta = 1$ (the one we reported in the first section).
```{r, message = FALSE}
df_firm_utility_aggregated <- df_firm_utility %>% 
  group_by(R) %>% 
  summarise('eta = 0.5' = n())

df_firm_utility_new_aggregated <- df_firm_utility_new %>% 
  group_by(R) %>% 
  summarise('eta = 2' = n())

df_compare = left_join(df_firm_utility_aggregated, df_firm_utility_new_aggregated) %>% pivot_longer(-R, names_to = 'type', values_to = 'freq') 

# ggplot(df_compare, aes(x = R, y = freq, fill = type)) +
#     geom_bar(aes(fill = type), stat = 'identity', position = 'dodge')+
#     xlim(-21,21) 

ggplot(df_compare, aes(x = R, y = freq, fill = type)) +
    geom_line(aes(color = type), size = 1)+
    xlim(-21,21) 
```


